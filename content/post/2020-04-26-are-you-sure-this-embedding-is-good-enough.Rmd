---
title: 'Are You Sure This Embedding Is Good Enough?'
author: Tim Radtke
date: '2020-04-26'
slug: are-you-sure-this-embedding-is-good-enough
categories:
  - few-shot image classification
tags:
  - embedding
  - image classification
  - few-shot
---

```{r, message = FALSE, echo = FALSE}
set.seed(759205)

library(dplyr)
library(ggplot2)

mnist_path <- "/Users/timradtke/Dropbox/1DataAnalyses/mnist-few-shot/"

mnist_umap_fmnist_train <- readr::read_csv(paste0(mnist_path, "data/mnist_umap_fmnist_train.csv"))
mnist_umap_fmnist_test <- readr::read_csv(paste0(mnist_path, "data/mnist_umap_fmnist_test.csv"))
mnist_umap_mnist_train <- readr::read_csv(paste0(mnist_path, "data/mnist_umap_mnist_train.csv"))
mnist_umap_mnist_test <- readr::read_csv(paste0(mnist_path, "data/mnist_umap_mnist_test.csv"))

fmnist_umap_mnist_test <- readr::read_csv(paste0(mnist_path, "data/fmnist_umap_mnist_test.csv"))
```

```{r, echo = FALSE, message = FALSE}
mnist_umap_fmnist_test_examples_2 <- readr::read_csv(paste0(mnist_path, "data/mnist_umap_fmnist_test_examples_2.csv"))
mnist_umap_fmnist_test_examples_4 <- readr::read_csv(paste0(mnist_path, "data/mnist_umap_fmnist_test_examples_4.csv"))
mnist_umap_fmnist_test_examples_9 <- readr::read_csv(paste0(mnist_path, "data/mnist_umap_fmnist_test_examples_9.csv"))

mnist_umap_mnist_test_examples_2 <- readr::read_csv(paste0(mnist_path, "data/mnist_umap_mnist_test_examples_2.csv"))
mnist_umap_mnist_test_examples_4 <- readr::read_csv(paste0(mnist_path, "data/mnist_umap_mnist_test_examples_4.csv"))
mnist_umap_mnist_test_examples_9 <- readr::read_csv(paste0(mnist_path, "data/mnist_umap_mnist_test_examples_9.csv"))

fmnist_umap_fmnist_test_examples_2 <- readr::read_csv(paste0(mnist_path, "data/fmnist_umap_fmnist_test_examples_2.csv"))
fmnist_umap_fmnist_test_examples_4 <- readr::read_csv(paste0(mnist_path, "data/fmnist_umap_fmnist_test_examples_4.csv"))
fmnist_umap_fmnist_test_examples_9 <- readr::read_csv(paste0(mnist_path, "data/fmnist_umap_fmnist_test_examples_9.csv"))

fmnist_umap_mnist_test_examples_2 <- readr::read_csv(paste0(mnist_path, "data/fmnist_umap_mnist_test_examples_2.csv"))
fmnist_umap_mnist_test_examples_4 <- readr::read_csv(paste0(mnist_path, "data/fmnist_umap_mnist_test_examples_4.csv"))
fmnist_umap_mnist_test_examples_9 <- readr::read_csv(paste0(mnist_path, "data/fmnist_umap_mnist_test_examples_9.csv"))
```

```{r, echo = FALSE}
evaluate_knn_pred <- function(test_examples) {
  
  nn_pred <- test_examples %>%
    dplyr::mutate(dist = sqrt(((x1 - x1_example)^2 + 
                                 (x2 - x2_example)^2) / 2)) %>%
    dplyr::group_by(test_index, label) %>%
    dplyr::summarize(label_pred = label_example[dist == min(dist)]) %>%
    dplyr::ungroup()
  
  nn_pred$label_pred_random <- sample(0:9, size = nrow(nn_pred), 
                                      replace = TRUE)
  
  test_example_random <- test_examples %>%
    dplyr::group_by(test_index) %>%
    dplyr::sample_n(1) %>%
    dplyr::rename(label_example_random = label_example) %>%
    dplyr::ungroup() %>%
    dplyr::select(test_index, label_example_random)
  
  acc_per_class <- nn_pred %>%
    dplyr::inner_join(test_example_random, by = "test_index") %>%
    dplyr::group_by(label) %>%
    dplyr::summarize(acc = mean(label == label_pred),
                     acc_r = mean(label == label_pred_random),
                     acc_er = mean(label == label_example_random))
  
  acc <- nn_pred %>%
    dplyr::inner_join(test_example_random, by = "test_index") %>%
    dplyr::summarize(acc = mean(label == label_pred),
                     acc_r = mean(label == label_pred_random),
                     acc_er = mean(label == label_example_random))
  
  return(list(acc = acc, acc_per_class = acc_per_class))
}
```

Suppose you are given a data set of five images to train on, and then have to classify new images with your trained model. Five training samples are in general not sufficient to train a state-of-the-art image classification model, thus this problem is hard and earned it's own name: few-shot image classification. A lot has been written on few-shot image classification and complex approaches have been suggested.^[Though I cannot say that I'm familiar with the literature; so maybe these approaches are actually quite simple.] Tian et al. (2020), however, suggest it suffices to take an embedding from a different image classification task, extract features from the five images via the embedding, and to train the few-shot model on the embedded version of the five images. For prediction, the new image can be embedded as well and then predicted via the trained model. This alone beats all previous approaches:

> In this work, we show that a simple baseline: learning a supervised or self- supervised representation on the meta-training set, followed by training a linear classifier on top of this representation, outperforms state-of-the-art few-shot learning methods.

The premise of "[Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?](https://arxiv.org/abs/2003.11539)" by Tian et al. (2020) sounds too good to be true: They essentially follow the steps you might know from [fine-tuning of](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) or [transfer learning with](https://www.tensorflow.org/tutorials/images/transfer_learning) a pre-trained image classifier---with the only difference that they use *five* images to fit a new head for their model.

### A Naive Example with (Fashion-)MNIST and UMAP

To reason in more detail about the paper's premise, let's look at a simple example: Suppose we have five images from the [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) data set; are we able to predict a new image from the same data set if all we have are those five images and a pre-trained embedding? As embedding, we'll use a simple UMAP trained on a "similar" data set: MNIST. 

Since Fashion-MNIST was designed as drop-in replacement for MNIST, we are dealing in both cases with 28x28 grey-scale pixels. At the same time, it isn't obvious that the embedding trained on MNIST is a useful feature extraction device for Fashion-MNIST.

So the steps are as follows:

1) Train UMAP embedding on MNIST train data set^[We did not use the label information as in [metric learning](https://umap-learn.readthedocs.io/en/latest/supervised.html#training-with-labels-and-embedding-unlabelled-test-data-metric-learning-with-umap) though this would be a nice additional step to explore.]
2) Project the Fashion MNIST train and test data onto the trained embedding
3) Take five random samples from the Fashion MNIST train data and predict a sample from the Fashion MNIST test data using the nearest neighbor^[Here we ensure that exactly one of the five training samples has the same label as the test observation.]

As usual with UMAP, we get a useful embedding of the MNIST data set. Here, we show the MNIST test set that has been projected onto this embedding:

```{r, echo = FALSE, fig.cap = "10,000 samples from the MNIST train and test data, respectively, projected onto a UMAP embedding trained on the full MNIST train data set."}
mnist_umap_mnist_test$dataset <- "Test"
mnist_umap_mnist_train$dataset <- "Train"

mnist_umap_mnist_train %>%
  sample_n(10000) %>%
  bind_rows(mnist_umap_mnist_test) %>%
  mutate(label = as.factor(label)) %>%
  ggplot(aes(x = x1, y = x2, group = label, color = label)) +
  geom_point(alpha = 0.1) +
  labs(x = "x", y = "y") +
  theme_bw() +
  facet_wrap(~dataset) +
  theme(legend.position = "bottom")
```

We see that even the *MNIST* test data would not be predicted perfectly by a k-nearest neighbor classifier: Not only are some samples projected onto the "cluster" of a different class, but the margins between clusters are quite small. This makes it easy for a sample to fall into the correct cluster but to then be classified based on a neighbor from a neighboring cluster.

We can now project the Fashion MNIST data onto the same embedding to evaluate how good our classification performance is when all we have are five samples from Fashion MNIST and the pretrained MNIST embedding. In the following, **UMAP_1NN** corresponds to the approach of embedding the data before classification via 1-nearest-neighbor, while **RANDOM** assigns a random label from the 10 classes and **RANDOM_SHOT** picks the label randomly from one of the $n$ training examples.

```{r, echo = FALSE}
mf_eval <- evaluate_knn_pred(mnist_umap_fmnist_test_examples_4)

mf_eval$acc %>%
  knitr::kable(digits = 3, 
               col.names = c("UMAP_1NN", "RANDOM", "RANDOM_SHOT"),
               caption = "Accuracy on Fashion MNIST test set using five training examples from Fashion MNIST train of which one has the correct class. The UMAP embedding was trained on the MNIST train data.")
```

When we consider the accuracy by class, we observe that some classes are easier to predict than others.

```{r, echo = FALSE}
mf_eval$acc_per_class %>%
  knitr::kable(digits = 3, 
               col.names = c("CLASS", "UMAP_1NN", "RANDOM", "RANDOM_SHOT"),
               caption = "Accuracy by class on Fashion MNIST test set using five training examples from Fashion MNIST train of which one has the correct class. The UMAP embedding was trained on the MNIST train data.")
```

To put these results into context, we perform a couple of other combinations: We can vary the training data for the UMAP embedding (use either the MNIST or the Fashion MNIST training set), and then evaluate how well we can predict the corresponding test sets. Here, it is especially interesting to compare how well we can predict MNIST using an MNIST embedding, and Fashion MNIST using a Fashion MNIST embedding--this is essentially the best we can hope for when predicting Fashion MNIST using MNIST embedding, and MNIST using the Fashion MNIST embedding.

Additionally, we can vary the number of few shot examples. Note that we always include exactly one sample from the correct class to have any chance of "learning" the new concept. This means, however, that with two samples we have a 50-50 chance of randomly assigning the correct class. Thus it's important to compare the **UMAP_1NN** performance against the performance of picking a random sample from the few shots (column `RS`).

Looking at the results, there are a few things to note.

First, observe how the performance generally degredates as the number of few shot training examples increases: Apparently this increases the chances of picking the wrong neighbor, since the share of training examples with the correct class drops from 50% to 20% to 10%---this is also seen in the `RS` column. As we add more training examples, chances increase that an example from a wrong class is embedded close to the test observation. If we could increase the training observations much more as well as the number of examples from the correct class, the performance should improve with $n$ as usually expected.

Note that we can predict Fashion MNIST better than random using the MNIST embedding; but we essentially predict randomly when classifying numbers given the Fashion MNIST embedding. This is somewhat surprising given that classifying Fashion MNIST is generally harder than classifying MNIST. Yet, the MNIST embedding appears more useful for the task at hand.

Also note that we can predict the test sets quite well if we use the embedding trained on the corresponding train set: We achieve a 60% accuracy for both Fashion MNIST and MNIST when using 10 training samples.^[Note that we sample the training examples from the test set: In theory, we might sometimes have the same observation in both the few shot examples and the observation to be classified. Given the sample sizes the chance is small, but nonetheless it might have given a slight positive bias.] Maybe we would actually expect better results given [how simple MNIST is](http://yann.lecun.com/exdb/mnist/)?

```{r, echo = FALSE}
results <- data.frame(
  umap_data = rep(c("MNIST Train", "F-MNIST Train"), each = 6),
  few_shot_train = rep(c("F-MNIST Train", "MNIST Test", 
                         "MNIST Train", "F-MNIST Test"), each = 3),
  few_shot_test = rep(c("F-MNIST Test", "MNIST Test", 
                         "MNIST Test", "F-MNIST Test"), each = 3),
  n_shots = rep(c(2,5,10), 4),
  umap_1nn = NA,
  random = NA,
  random_shot = NA
)

results[1,5:7] <- evaluate_knn_pred(mnist_umap_fmnist_test_examples_2)$acc
results[2,5:7] <- evaluate_knn_pred(mnist_umap_fmnist_test_examples_4)$acc
results[3,5:7] <- evaluate_knn_pred(mnist_umap_fmnist_test_examples_9)$acc

results[4,5:7] <- evaluate_knn_pred(mnist_umap_mnist_test_examples_2)$acc
results[5,5:7] <- evaluate_knn_pred(mnist_umap_mnist_test_examples_4)$acc
results[6,5:7] <- evaluate_knn_pred(mnist_umap_mnist_test_examples_9)$acc

results[7,5:7] <- evaluate_knn_pred(fmnist_umap_mnist_test_examples_2)$acc
results[8,5:7] <- evaluate_knn_pred(fmnist_umap_mnist_test_examples_4)$acc
results[9,5:7] <- evaluate_knn_pred(fmnist_umap_mnist_test_examples_9)$acc

results[10,5:7] <- evaluate_knn_pred(fmnist_umap_fmnist_test_examples_2)$acc
results[11,5:7] <- evaluate_knn_pred(fmnist_umap_fmnist_test_examples_4)$acc
results[12,5:7] <- evaluate_knn_pred(fmnist_umap_fmnist_test_examples_9)$acc
```

```{r, echo = FALSE}
results %>%
  knitr::kable(digits = 2,
               col.names = c("UMAP", "TRAIN", "TEST", "n",
                             "UMAP_1NN", "R", "RS"))
```

***

```{r, echo = FALSE, fig.cap="This figure shows the MNIST test set projected onto a UMAP embedding trained on the Fashion MNIST train data. Also, the Fashion MNIST test set projected onto a UMAP embedding trained on the MNIST train data. In contrast to figure 1 above, we see that the projection *across* data sets does not work too well."}
mnist_umap_fmnist_test$dataset <- "Fashion MNIST"
fmnist_umap_mnist_test$dataset <- "MNIST"

bind_rows(mnist_umap_fmnist_test, fmnist_umap_mnist_test) %>%
  mutate(label = as.factor(label)) %>%
  ggplot(aes(x = x1, y = x2, group = label, color = label)) +
  geom_point(alpha = 0.1) +
  labs(x = "x", y = "y") +
  theme_bw() +
  facet_wrap(~dataset) +
  theme(legend.position = "bottom")
```

### Questions Raised by This Approach

Let me be clear that my quick'n'dirty experiment above does not follow the procedure of Tian et al. (2020) too closely: They use a CNN trained for classification as feature extractor ("embedding") which probably makes more sense than UMAP to embed new "unseen" classes (it also returns more dimensions than just the two I have). Also, I use MNIST and Fashion MNIST which are not used for few-shot learning evaluation.

Still, I wonder: If I would be faced with an "actual" few-shot learning task in the real world, would I like to rely on some embedding? How do I know that the feature extractor will extract features from my few training samples in a useful way? I certainly wouldn't have sufficient samples to evaluate the quality of the (pre-trained) embedding for my new images. 

Indeed, the evaluation setup used in Tian et al. (2020) (and, most likely, in the remaining few-shot learning literature) feels too easy: The evaluation only checks whether an approach that has access to a large set of training samples from ImageNet (e.g., bike, panda, cat, ...) can learn to classify new classes from ImageNet (e.g., car, chair, dog, ...) from few samples. What surprises is me is how related the images are allowed to be---the test of generalization does not appear difficult enough. It's not like we allow some pre-training on ImageNet and then try to classify different diseases based on CT scans.

The code used to produce these results is [available on Github](https://github.com/timradtke/mnist-few-shot).

### References

Thomas Cover, P.E. Hart (1967). *Nearest Neighbor Pattern Classification*. IEEE Transactions on Information Theory, Vol. IT-13, No. 1.

Yann LeCun, Corinna Cortes, Christopher J.C. Burges. *The MNIST database of handwritten digits*. http://yann.lecun.com/exdb/mnist/

Leland McInnes, John Healy, James Melville (2018). *UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction*. https://arxiv.org/abs/1802.03426

Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenenbaum, Phillip Isola (2020). *Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?*. https://arxiv.org/abs/2003.11539

Han Xiao, Kashif Rasul, Roland Vollgraf (2017). *Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms*. https://arxiv.org/abs/1708.07747
